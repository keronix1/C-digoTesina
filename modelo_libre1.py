# -*- coding: utf-8 -*-
"""Modelo_libre1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlTUXtrMdVCocd5-9GItnhAOtNvotes4
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import random_split
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import ReduceLROnPlateau

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(f'Using device: ', {device})

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/My Drive/CanalesCorrectosImagenes'

!ls '/content/drive/My Drive/CanalesCorrectosImagenes'

"""Sirve para transformar imágenes en pytorch
*toTensor: para que pueda ser procesada
*Normaliza los valores de los píxeles de la imagen
"""

transform = transforms.Compose([
        transforms.Resize([224,224]),
        transforms.ToTensor(),
        transforms.Normalize([0.229], [0.225])
    ])

"""crea un conjunto de datos de la imagen apartir ImageFolder, path=ruta, y se define una secuencia de transformaciones de imágenes con Compose."""

full_dataset = datasets.ImageFolder(path, transform=transform)

"""*TRAIN_SIZE: Sirve para definir como el 80% del tamaño total del conjunto de datos, se utilizará para entrenar al modelo. (No puedo usar todo?)
*VAL_SIZE y TEST_SIZE: se definen como el 10% del tamaño total del conjunto de datos cada uno
*BATCH_SIZE: se define como 16, el modelo entrenará en lotes de 16 imágenes a la vez, * Puedo cambiarlo por uno menor?
"""

TRAIN_SIZE =  int(len(full_dataset) * 0.8)
VAL_SIZE = int(len(full_dataset) * 0.1 + 1)
TEST_SIZE = int(len(full_dataset) * 0.1 + 1)
BATCH_SIZE = 16

"""
*Sirve para dividirlo en tres conjuntos, de training, validación y pruebas.
*Random_split, devuelve una lista de forma aleatoria


"""

training_set, val_set, test_set = random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE, TEST_SIZE])

"""Genera los lotes correspondientes a cada una de las variables"""

training_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)

# categories = ['genu varo', 'genu valgo']
categories = ['genu varo', 'genu valgo']
def plot_figure(image):
    plt.imshow(image.permute(1,2,0))
    plt.show()

#Sirve para seleccionar una imagen del conjunto de datos
rnd_sample_idx = np.random.randint(TEST_SIZE)
print(f'La imagen muestreada representa un: {categories[test_loader.dataset[rnd_sample_idx][1]]}')

image = test_loader.dataset[rnd_sample_idx][0]
image = (image - image.min()) / (image.max() -image.min() )
plot_figure(image)

"""Sirve para calcular el accuracy del modelo"""

def accuracy(model, loader):
    num_correct = 0
    num_total = 0
    #establece un modelo de evaluación
    model.eval()
    #sirve para que el modelo se mueva a la GPU
    model = model.to(device=device)
    with torch.no_grad():
        for (xi, yi) in loader:
            xi = xi.to(device=device, dtype = torch.float32)
            yi = yi.to(device=device, dtype = torch.long)
            #modelo se utiliza para predecir las etiquetas de clase para los datos de entrada 
            scores = model(xi) # mb_size, 10
            _, pred = scores.max(dim=1) #pred shape (mb_size )
            num_correct += (pred == yi.squeeze()).sum() # pred shape (mb_size), yi shape (mb_size, 1)
            num_total += pred.size(0)

        return float(num_correct)/num_total

def train(model, optimiser, epochs=100):
#     def train(model, optimiser, scheduler = None, epochs=100):
    model = model.to(device=device)
    for epoch in range(epochs):
      #se iteran el conjunto de datos y se ajusta el modelo y se mueven los datos de entrada y salida
        for (xi, yi) in training_loader:
            model.train()
            xi = xi.to(device=device, dtype=torch.float32)
            yi = yi.to(device=device, dtype=torch.long)
            scores = model(xi)

            #F.cross es una funcion de perdida que se utiliza para la clasificación multiclase
            #cost se utiliza posteriormente para ajustar los pesos del modelo utilizando el optimizador
            cost = F.cross_entropy(input= scores, target=yi.squeeze())
        
            optimiser.zero_grad() #borra los gradientes acumulados de los parametros      
            cost.backward() #calculan los gradientes de los parametros del modelo
            optimiser.step() #actualiza los parametros del modelo en funcion de los gradientes     
            
        acc = accuracy(model, val_loader)
        if epoch%5 == 0:     
            print(f'Epoch: {epoch}, costo: {cost.item()}, accuracy: {acc},')
#         scheduler.step()

from sklearn.metrics import accuracy_score, f1_score
epochs = 25

lr_values = []
accuracy_values = []
f1_values = []

validation_results = []

best_lr = None
best_accuracy = 0.0
best_f1 = 0.0

for _ in range(5):
    lr = np.random.uniform(0.001, 0.1)
    lr_values.append(lr)
    
    model = nn.Sequential(
        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,
                 padding=1, stride=1, bias=True),
        nn.BatchNorm2d(num_features=32),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),

        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,
                 padding=1, stride=1, bias=True),
        nn.BatchNorm2d(num_features=64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),

        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,
                 padding=1, stride=1, bias=True),
        nn.BatchNorm2d(num_features=128),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),

        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,
                 padding=1, stride=1, bias=True),
        nn.BatchNorm2d(num_features=128),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),

        nn.Flatten(),
        nn.Linear(in_features=14 * 14 * 128, out_features=2, bias=True)
    )

    optimizer = optim.Adam(params=model.parameters(), lr=lr)
    train(model, optimizer, epochs)

    true_labels = []  # Etiquetas verdaderas
    predicted_labels = []  # Etiquetas predichas

    for val_data in val_loader:
        input_batch, true_label_batch = val_data
        input_batch = input_batch.to(device)
        true_label_batch = true_label_batch.to(device)

        # Realizar predicciones
        model.eval()
        with torch.no_grad():
            predicted_label_batch = model(input_batch)
            predicted_label_batch = torch.argmax(predicted_label_batch, dim=1)

        # Guardar etiquetas verdaderas y predichas
        true_labels.extend(true_label_batch.tolist())
        predicted_labels.extend(predicted_label_batch.tolist())

    acc = accuracy_score(true_labels, predicted_labels)
    f1 = f1_score(true_labels, predicted_labels)

    accuracy_values.append(acc)
    f1_values.append(f1)

    validation_results.append({'Learning Rate': lr, 'Accuracy': acc, 'F1 Score': f1})

    print(f'LR: {lr}, Accuracy: {acc}, F1 Score: {f1}')

    if acc > best_accuracy:
        best_lr = lr
        best_accuracy = acc
        best_f1 = f1


results = pd.DataFrame(validation_results)
results.index = range(1, len(results) + 1)
print(results)

epochs = 55
num_classes = 2
optimizer = optim.Adam(params = model.parameters(), lr= lr)
train(model, optimizer, epochs)
acc = accuracy(model, val_loader)







